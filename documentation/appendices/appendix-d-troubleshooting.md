# ðŸ“– Appendix D - Troubleshooting Guide

## ðŸš¨ Common Issues and Solutions

### Database Connectivity Issues

#### Problem: Application Cannot Connect to Database
```yaml
Symptoms:
  - Connection timeout errors
  - "Access denied" messages
  - SSL/TLS handshake failures
  - DNS resolution errors
  
Diagnostic Commands:
  # Test network connectivity
  telnet xyz-corp-primary-db.cluster-xyz.us-east-1.rds.amazonaws.com 3306
  
  # Check DNS resolution
  nslookup xyz-corp-primary-db.cluster-xyz.us-east-1.rds.amazonaws.com
  
  # Test SSL connection
  mysql -h xyz-corp-primary-db.cluster-xyz.us-east-1.rds.amazonaws.com -u admin -p --ssl-mode=REQUIRED
  
Common Causes & Solutions:
  1. Security Group Misconfiguration:
     - Check inbound rules on RDS security group
     - Verify application security group is allowed
     - Ensure port 3306 is open for MySQL
  
  2. VPC/Subnet Issues:
     - Verify RDS is in correct subnet group
     - Check route tables for connectivity
     - Ensure NAT Gateway/Internet Gateway configuration
  
  3. SSL/TLS Certificate Issues:
     - Update RDS CA certificate (rds-ca-2019)
     - Configure application for SSL requirements
     - Check certificate expiration dates
  
  4. Authentication Problems:
     - Verify username/password combination
     - Check IAM database authentication setup
     - Validate user permissions in MySQL
```

#### Problem: Connection Pool Exhaustion
```yaml
Symptoms:
  - "Too many connections" errors
  - Application hangs on database operations
  - High connection count metrics
  - Connection timeout errors
  
Diagnostic Queries:
  # Check current connections
  SHOW PROCESSLIST;
  
  # Check connection statistics
  SHOW STATUS LIKE 'Threads_connected';
  SHOW STATUS LIKE 'Max_used_connections';
  SHOW VARIABLES LIKE 'max_connections';
  
  # Identify long-running queries
  SELECT 
    ID, USER, HOST, DB, COMMAND, TIME, STATE, INFO 
  FROM INFORMATION_SCHEMA.PROCESSLIST 
  WHERE TIME > 60 
  ORDER BY TIME DESC;
  
Solutions:
  1. Application-Level Fixes:
     - Implement proper connection pooling
     - Reduce connection pool size per instance
     - Add connection timeout configurations
     - Fix connection leaks in application code
  
  2. Database-Level Adjustments:
     - Increase max_connections parameter
     - Tune wait_timeout and interactive_timeout
     - Monitor for blocking queries
     - Implement connection limiting
  
  3. Architecture Changes:
     - Add connection pooler (like pgbouncer equivalent)
     - Implement read/write splitting
     - Scale horizontally with read replicas
```

### Performance Issues

#### Problem: Slow Query Performance
```yaml
Symptoms:
  - High query response times
  - Application timeouts
  - High CPU utilization
  - Poor user experience
  
Diagnostic Process:
  1. Enable and Review Slow Query Log:
     SET GLOBAL slow_query_log = 'ON';
     SET GLOBAL long_query_time = 1;
     
     # Review slow query log
     tail -f /rds/log/slowquery/mysql-slowquery.log
  
  2. Use Performance Schema:
     SELECT 
       DIGEST_TEXT,
       COUNT_STAR as exec_count,
       AVG_TIMER_WAIT/1000000000 as avg_seconds,
       SUM_TIMER_WAIT/1000000000 as total_seconds
     FROM performance_schema.events_statements_summary_by_digest 
     ORDER BY SUM_TIMER_WAIT DESC LIMIT 10;
  
  3. Analyze Query Execution Plans:
     EXPLAIN ANALYZE SELECT * FROM device_data WHERE timestamp > '2024-01-01';
  
Common Solutions:
  1. Index Optimization:
     # Add missing indexes
     CREATE INDEX idx_device_timestamp ON device_data(device_id, timestamp);
     
     # Remove unused indexes
     DROP INDEX unused_index ON table_name;
     
     # Analyze index usage
     SELECT 
       OBJECT_NAME, INDEX_NAME, COUNT_STAR 
     FROM performance_schema.table_io_waits_summary_by_index_usage 
     WHERE COUNT_STAR = 0;
  
  2. Query Rewriting:
     # Before (inefficient)
     SELECT * FROM device_data WHERE DATE(timestamp) = '2024-01-15';
     
     # After (efficient)
     SELECT * FROM device_data 
     WHERE timestamp >= '2024-01-15 00:00:00' 
       AND timestamp < '2024-01-16 00:00:00';
  
  3. Parameter Tuning:
     # Increase buffer pool size
     innodb_buffer_pool_size = 3GB  # 75% of available RAM
     
     # Optimize query cache
     query_cache_size = 256MB
     query_cache_limit = 2MB
```

#### Problem: High Database CPU Utilization
```yaml
Symptoms:
  - CPU metrics consistently above 80%
  - Slow application response times
  - Database connection timeouts
  - Poor overall performance
  
Diagnostic Steps:
  1. Identify Resource-Intensive Queries:
     SELECT 
       DIGEST_TEXT,
       SUM_TIMER_WAIT/1000000000 as total_seconds,
       AVG_TIMER_WAIT/1000000000 as avg_seconds,
       COUNT_STAR as exec_count
     FROM performance_schema.events_statements_summary_by_digest 
     ORDER BY SUM_TIMER_WAIT DESC LIMIT 20;
  
  2. Check for Blocking Operations:
     SELECT 
       r.trx_id waiting_trx_id,
       r.trx_mysql_thread_id waiting_thread,
       r.trx_query waiting_query,
       b.trx_id blocking_trx_id,
       b.trx_mysql_thread_id blocking_thread,
       b.trx_query blocking_query
     FROM information_schema.innodb_lock_waits w
     INNER JOIN information_schema.innodb_trx b ON b.trx_id = w.blocking_trx_id
     INNER JOIN information_schema.innodb_trx r ON r.trx_id = w.requesting_trx_id;
  
  3. Monitor Database Load:
     SHOW ENGINE INNODB STATUS\G
  
Solutions:
  1. Query Optimization:
     - Add appropriate indexes
     - Rewrite inefficient queries
     - Implement query result caching
     - Use LIMIT clauses for large result sets
  
  2. Configuration Tuning:
     # Increase InnoDB buffer pool
     innodb_buffer_pool_size = 75% of RAM
     
     # Optimize thread handling
     thread_cache_size = 128
     
     # Adjust query cache
     query_cache_type = ON
     query_cache_size = 512MB
  
  3. Architecture Changes:
     - Offload read queries to read replicas
     - Implement connection pooling
     - Scale up to larger instance type
     - Consider database sharding
```

### Cache-Related Issues

#### Problem: Low Cache Hit Rate
```yaml
Symptoms:
  - Cache hit rate below 85%
  - High application response times
  - Increased database load
  - Poor user experience
  
Diagnostic Commands:
  # Redis cache statistics
  redis-cli info stats
  
  # Check memory usage
  redis-cli info memory
  
  # Monitor key expiration
  redis-cli info keyspace
  
  # Check for frequent evictions
  redis-cli --latency -h your-redis-endpoint
  
Common Causes & Solutions:
  1. Insufficient Cache Memory:
     # Check memory usage
     redis-cli info memory | grep used_memory_human
     
     Solutions:
     - Increase cache cluster size
     - Optimize data structures
     - Implement better TTL strategies
     - Remove unnecessary cached data
  
  2. Poor TTL Strategy:
     # Analyze key expiration patterns
     redis-cli --scan --pattern "device:*" | head -100 | xargs redis-cli ttl
     
     Solutions:
     - Adjust TTL values based on access patterns
     - Implement sliding expiration
     - Use different TTL for different data types
  
  3. Cache Key Design Issues:
     # Check key distribution
     redis-cli --scan | head -1000 | cut -d: -f1-2 | sort | uniq -c
     
     Solutions:
     - Redesign cache key structure
     - Implement better invalidation strategies
     - Use hash tags for related data
```

#### Problem: Cache Connection Issues
```yaml
Symptoms:
  - Redis connection timeouts
  - Application falling back to database
  - High error rates in logs
  - Inconsistent cache performance
  
Diagnostic Steps:
  1. Test Cache Connectivity:
     redis-cli -h your-cache-endpoint ping
     redis-cli -h your-cache-endpoint info clients
  
  2. Check Network Latency:
     redis-cli -h your-cache-endpoint --latency
     redis-cli -h your-cache-endpoint --latency-history
  
  3. Monitor Connection Pool:
     # Application-specific connection pool metrics
     # Check for connection pool exhaustion
  
Solutions:
  1. Network Configuration:
     - Verify security group rules
     - Check VPC and subnet configuration
     - Test cross-AZ connectivity
     - Validate DNS resolution
  
  2. Connection Pool Optimization:
     # Optimize connection pool settings
     max_connections = 50
     connection_timeout = 5 seconds
     socket_keepalive = True
     retry_on_timeout = True
  
  3. Redis Configuration:
     # Adjust Redis timeouts
     timeout = 300
     tcp-keepalive = 60
     tcp-backlog = 511
```

### Real-Time Processing Issues

#### Problem: Kinesis Stream Processing Delays
```yaml
Symptoms:
  - High end-to-end processing latency
  - Growing shard iterator age
  - Consumer lag alerts
  - Data processing delays
  
Diagnostic Commands:
  # Check stream metrics
  aws kinesis describe-stream --stream-name xyz-corp-device-data-stream
  
  # Monitor shard iterator age
  aws cloudwatch get-metric-statistics \
    --namespace AWS/Kinesis \
    --metric-name IncomingRecords \
    --dimensions Name=StreamName,Value=xyz-corp-device-data-stream
  
  # Check consumer application metrics
  aws kinesis list-shards --stream-name xyz-corp-device-data-stream
  
Common Causes & Solutions:
  1. Insufficient Shard Capacity:
     # Calculate required shards
     Required shards = (Records per second / 1000) + (Data MB per second / 1)
     
     Solutions:
     - Split shards to increase capacity
     - Implement auto-scaling for shards
     - Optimize record batching
     - Compress record payloads
  
  2. Consumer Processing Issues:
     Solutions:
     - Optimize consumer processing logic
     - Implement parallel processing
     - Add error handling and retries
     - Scale consumer instances
  
  3. Downstream Bottlenecks:
     Solutions:
     - Optimize DynamoDB write capacity
     - Implement batch writing
     - Add downstream caching
     - Parallelize downstream processing
```

#### Problem: DynamoDB Throttling
```yaml
Symptoms:
  - ProvisionedThroughputExceeded errors
  - High latency for read/write operations
  - Application timeouts
  - Data processing delays
  
Diagnostic Queries:
  # Check table metrics
  aws dynamodb describe-table --table-name xyz-corp-realtime-analytics
  
  # Monitor throttling events
  aws logs filter-log-events \
    --log-group-name /aws/dynamodb/table/xyz-corp-realtime-analytics \
    --filter-pattern "ProvisionedThroughputExceededException"
  
Solutions:
  1. Partition Key Optimization:
     # Analyze access patterns
     aws dynamodb query \
       --table-name xyz-corp-realtime-analytics \
       --key-condition-expression "PK = :pk" \
       --expression-attribute-values '{":pk":{"S":"device:123"}}'
     
     Solutions:
     - Redesign partition keys for better distribution
     - Add random suffix to keys
     - Implement write sharding
     - Use composite partition keys
  
  2. Capacity Management:
     Solutions:
     - Enable auto-scaling
     - Increase provisioned capacity temporarily
     - Switch to on-demand billing mode
     - Implement exponential backoff in application
  
  3. Access Pattern Optimization:
     Solutions:
     - Implement batch operations
     - Use consistent reads only when necessary
     - Cache frequently accessed items
     - Optimize GSI usage patterns
```

## ðŸ”§ Diagnostic Tools and Commands

### Database Diagnostics

#### MySQL Performance Analysis
```sql
-- Check database status
SHOW ENGINE INNODB STATUS\G

-- Analyze slow queries
SELECT 
    SCHEMA_NAME as db,
    DIGEST_TEXT as query,
    COUNT_STAR as exec_count,
    ROUND(AVG_TIMER_WAIT/1000000000,2) as avg_time,
    ROUND(SUM_TIMER_WAIT/1000000000,2) as total_time
FROM performance_schema.events_statements_summary_by_digest 
WHERE SCHEMA_NAME IS NOT NULL 
ORDER BY total_time DESC 
LIMIT 20;

-- Check index usage
SELECT 
    OBJECT_SCHEMA,
    OBJECT_NAME,
    INDEX_NAME,
    COUNT_STAR,
    SUM_TIMER_WAIT/1000000000 as total_time
FROM performance_schema.table_io_waits_summary_by_index_usage 
WHERE OBJECT_SCHEMA NOT IN ('mysql', 'performance_schema') 
ORDER BY SUM_TIMER_WAIT DESC;

-- Monitor replication lag
SHOW SLAVE STATUS\G

-- Check connection information
SELECT 
    USER,
    HOST,
    DB,
    COMMAND,
    TIME,
    STATE,
    LEFT(INFO, 100) as QUERY_START
FROM INFORMATION_SCHEMA.PROCESSLIST 
ORDER BY TIME DESC;

-- Analyze table statistics
SELECT 
    TABLE_SCHEMA,
    TABLE_NAME,
    ENGINE,
    TABLE_ROWS,
    DATA_LENGTH/1024/1024 as DATA_MB,
    INDEX_LENGTH/1024/1024 as INDEX_MB
FROM INFORMATION_SCHEMA.TABLES 
WHERE TABLE_SCHEMA NOT IN ('information_schema','mysql','performance_schema')
ORDER BY DATA_LENGTH DESC;
```

#### RDS CloudWatch Metrics Analysis
```bash
# Database CPU utilization
aws cloudwatch get-metric-statistics \
  --namespace AWS/RDS \
  --metric-name CPUUtilization \
  --dimensions Name=DBInstanceIdentifier,Value=xyz-corp-primary-db \
  --start-time 2024-01-15T00:00:00Z \
  --end-time 2024-01-15T23:59:59Z \
  --period 300 \
  --statistics Average,Maximum

# Database connections
aws cloudwatch get-metric-statistics \
  --namespace AWS/RDS \
  --metric-name DatabaseConnections \
  --dimensions Name=DBInstanceIdentifier,Value=xyz-corp-primary-db \
  --start-time 2024-01-15T00:00:00Z \
  --end-time 2024-01-15T23:59:59Z \
  --period 300 \
  --statistics Average,Maximum

# Read replica lag
aws cloudwatch get-metric-statistics \
  --namespace AWS/RDS \
  --metric-name ReplicaLag \
  --dimensions Name=DBInstanceIdentifier,Value=xyz-corp-replica-west \
  --start-time 2024-01-15T00:00:00Z \
  --end-time 2024-01-15T23:59:59Z \
  --period 300 \
  --statistics Average,Maximum
```

### Cache Diagnostics

#### Redis Analysis Commands
```bash
# Cache statistics
redis-cli info stats

# Memory usage analysis
redis-cli info memory

# Connection information
redis-cli info clients

# Key distribution analysis
redis-cli --scan --pattern "*" | head -1000 | cut -d: -f1-2 | sort | uniq -c | sort -nr

# Monitor cache operations in real-time
redis-cli monitor

# Check specific key TTL
redis-cli ttl "device:123:data:current"

# Analyze slow operations
redis-cli --latency -i 1

# Check keyspace information
redis-cli info keyspace

# Memory usage by key pattern
redis-cli --memkeys --memkeys-samples 1000
```

#### ElastiCache CloudWatch Metrics
```bash
# Cache hit rate
aws cloudwatch get-metric-statistics \
  --namespace AWS/ElastiCache \
  --metric-name CacheHits \
  --dimensions Name=CacheClusterId,Value=xyz-corp-redis-primary \
  --start-time 2024-01-15T00:00:00Z \
  --end-time 2024-01-15T23:59:59Z \
  --period 300 \
  --statistics Sum

# CPU utilization
aws cloudwatch get-metric-statistics \
  --namespace AWS/ElastiCache \
  --metric-name CPUUtilization \
  --dimensions Name=CacheClusterId,Value=xyz-corp-redis-primary \
  --start-time 2024-01-15T00:00:00Z \
  --end-time 2024-01-15T23:59:59Z \
  --period 300 \
  --statistics Average,Maximum

# Evicted keys
aws cloudwatch get-metric-statistics \
  --namespace AWS/ElastiCache \
  --metric-name Evictions \
  --dimensions Name=CacheClusterId,Value=xyz-corp-redis-primary \
  --start-time 2024-01-15T00:00:00Z \
  --end-time 2024-01-15T23:59:59Z \
  --period 300 \
  --statistics Sum
```

### Real-Time Processing Diagnostics

#### Kinesis Stream Analysis
```bash
# Stream description
aws kinesis describe-stream --stream-name xyz-corp-device-data-stream

# List shards
aws kinesis list-shards --stream-name xyz-corp-device-data-stream

# Get shard iterator
aws kinesis get-shard-iterator \
  --stream-name xyz-corp-device-data-stream \
  --shard-id shardId-000000000000 \
  --shard-iterator-type LATEST

# Monitor stream metrics
aws cloudwatch get-metric-statistics \
  --namespace AWS/Kinesis \
  --metric-name IncomingRecords \
  --dimensions Name=StreamName,Value=xyz-corp-device-data-stream \
  --start-time 2024-01-15T00:00:00Z \
  --end-time 2024-01-15T23:59:59Z \
  --period 300 \
  --statistics Sum,Average

# Check for throttling
aws logs filter-log-events \
  --log-group-name /aws/kinesis/xyz-corp-device-data-stream \
  --filter-pattern "ProvisionedThroughputExceededException"
```

#### DynamoDB Diagnostics
```bash
# Table description
aws dynamodb describe-table --table-name xyz-corp-realtime-analytics

# Check table metrics
aws cloudwatch get-metric-statistics \
  --namespace AWS/DynamoDB \
  --metric-name ConsumedReadCapacityUnits \
  --dimensions Name=TableName,Value=xyz-corp-realtime-analytics \
  --start-time 2024-01-15T00:00:00Z \
  --end-time 2024-01-15T23:59:59Z \
  --period 300 \
  --statistics Sum,Average

# Monitor throttling events
aws cloudwatch get-metric-statistics \
  --namespace AWS/DynamoDB \
  --metric-name ReadThrottledEvents \
  --dimensions Name=TableName,Value=xyz-corp-realtime-analytics \
  --start-time 2024-01-15T00:00:00Z \
  --end-time 2024-01-15T23:59:59Z \
  --period 300 \
  --statistics Sum

# Scan table for hot partitions
aws dynamodb scan \
  --table-name xyz-corp-realtime-analytics \
  --projection-expression "PK" \
  --max-items 1000 | \
  jq -r '.Items[].PK.S' | \
  sort | uniq -c | sort -nr | head -20
```

## ðŸš¨ Emergency Response Procedures

### Database Failover Procedures

#### Multi-AZ Automatic Failover Response
```yaml
When Automatic Failover Occurs:
  1. Monitor and Verify (0-2 minutes):
     - Check CloudWatch alarms
     - Verify application error rates
     - Monitor database endpoint connectivity
     
  2. Application Health Check (2-5 minutes):
     - Test application database connectivity
     - Verify read/write operations
     - Check application error logs
     
  3. Performance Validation (5-15 minutes):
     - Monitor response times
     - Check query performance
     - Validate backup systems
     
  4. Communication (Throughout):
     - Update status page
     - Notify stakeholders
     - Document incident timeline

Emergency Commands:
  # Force manual failover (if needed)
  aws rds reboot-db-instance \
    --db-instance-identifier xyz-corp-primary-db \
    --force-failover

  # Check failover status
  aws rds describe-db-instances \
    --db-instance-identifier xyz-corp-primary-db \
    --query 'DBInstances[0].DBInstanceStatus'
```

#### Read Replica Promotion (Regional Failure)
```yaml
Manual Promotion Process:
  1. Assess Situation (0-3 minutes):
     - Confirm primary region unavailability
     - Identify best read replica for promotion
     - Calculate potential data loss (RPO)
     
  2. Execute Promotion (3-10 minutes):
     # Promote read replica
     aws rds promote-read-replica \
       --db-instance-identifier xyz-corp-replica-west
     
  3. Update Application Configuration (10-20 minutes):
     - Update database endpoints
     - Modify connection strings
     - Deploy configuration changes
     - Restart application services
     
  4. Verify Operations (20-30 minutes):
     - Test all application functions
     - Verify data integrity
     - Check replication setup
     
  5. Communication and Documentation:
     - Update stakeholders
     - Document lessons learned
     - Plan recovery to primary region
```

### Cache Cluster Failure Response

#### Redis Cluster Failover
```yaml
Automatic Failover Response:
  1. Immediate Assessment (0-2 minutes):
     - Verify cache cluster status
     - Check application fallback to database
     - Monitor database load increase
     
  2. Application Performance Check (2-5 minutes):
     - Monitor response times
     - Check error rates
     - Verify database capacity
     
  3. Cache Recovery (5-15 minutes):
     - Wait for automatic failover completion
     - Test cache connectivity
     - Verify data integrity
     
  4. Cache Warming (15-30 minutes):
     - Execute cache warming procedures
     - Monitor hit rate recovery
     - Validate performance metrics

Manual Intervention (if automatic fails):
  # Check cluster status
  aws elasticache describe-replication-groups \
    --replication-group-id xyz-corp-redis-primary
  
  # Force failover if needed
  aws elasticache test-failover \
    --replication-group-id xyz-corp-redis-primary \
    --node-group-id 0001
```

### Performance Degradation Response

#### High Database CPU Response
```yaml
Immediate Actions (0-5 minutes):
  1. Identify root cause:
     # Check for blocking queries
     SHOW PROCESSLIST;
     
     # Look for long-running transactions
     SELECT * FROM information_schema.innodb_trx 
     WHERE trx_started < NOW() - INTERVAL 5 MINUTE;
  
  2. Quick fixes:
     # Kill problematic queries (if safe)
     KILL QUERY process_id;
     
     # Restart read-heavy processes
     # Route traffic to read replicas

Sustained Response (5-30 minutes):
  1. Scale resources:
     # Modify instance class (temporary)
     aws rds modify-db-instance \
       --db-instance-identifier xyz-corp-primary-db \
       --db-instance-class db.t3.large \
       --apply-immediately
  
  2. Optimize workload:
     - Enable query caching
     - Route reads to replicas
     - Implement connection pooling
  
  3. Long-term planning:
     - Analyze query patterns
     - Plan capacity upgrades
     - Review architecture
```

#### Cache Performance Degradation
```yaml
Immediate Response (0-5 minutes):
  1. Check cache cluster health:
     redis-cli -h cache-endpoint ping
     redis-cli -h cache-endpoint info stats
  
  2. Identify issues:
     # Check memory usage
     redis-cli info memory | grep used_memory_human
     
     # Check for evictions
     redis-cli info stats | grep evicted_keys
  
  3. Quick remediation:
     # Scale up cache cluster
     aws elasticache modify-replication-group \
       --replication-group-id xyz-corp-redis-primary \
       --cache-node-type cache.t3.small \
       --apply-immediately
     
     # Clear problematic keys (if identified)
     redis-cli del problematic_key_pattern*
```

## ðŸ“š Troubleshooting Checklists

### Daily Health Checks
```yaml
Database Health:
  â–¡ Check CPU utilization < 75%
  â–¡ Verify memory usage < 85%
  â–¡ Confirm connection count < 90% of max
  â–¡ Check for slow queries > 2 seconds
  â–¡ Verify backup completion
  â–¡ Check replica lag < 5 seconds

Cache Health:
  â–¡ Verify hit rate > 90%
  â–¡ Check memory utilization < 85%
  â–¡ Confirm connection pool health
  â–¡ Check for evictions
  â–¡ Verify cluster connectivity
  â–¡ Test failover readiness

Real-Time Processing:
  â–¡ Check Kinesis stream health
  â–¡ Verify consumer lag < 1 minute
  â–¡ Check DynamoDB throttling events
  â–¡ Confirm error rates < 0.1%
  â–¡ Test end-to-end processing
  â–¡ Verify data integrity
```

### Incident Response Checklist
```yaml
Immediate Response (0-15 minutes):
  â–¡ Acknowledge incident alerts
  â–¡ Assess severity and impact
  â–¡ Form incident response team
  â–¡ Begin preliminary investigation
  â–¡ Implement immediate mitigations
  â–¡ Update status page (if applicable)

Investigation Phase (15-60 minutes):
  â–¡ Gather diagnostic information
  â–¡ Identify root cause
  â–¡ Determine fix timeline
  â–¡ Implement temporary workarounds
  â–¡ Monitor system stability
  â–¡ Communicate with stakeholders

Resolution Phase (1-4 hours):
  â–¡ Implement permanent fix
  â–¡ Test fix effectiveness
  â–¡ Verify system stability
  â–¡ Monitor for regression
  â–¡ Update documentation
  â–¡ Conduct post-incident review

Post-Incident (24-48 hours):
  â–¡ Complete incident report
  â–¡ Identify improvement actions
  â–¡ Update runbooks/procedures
  â–¡ Schedule follow-up reviews
  â–¡ Implement preventive measures
  â–¡ Update monitoring/alerting
```

---
*This appendix is part of the XYZ Corporation Multi-Database Architecture Case Study project.*